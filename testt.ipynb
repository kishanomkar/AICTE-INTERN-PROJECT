{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trash Classification with Focal Loss and EfficientNetV2B2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom, RandomContrast, RandomBrightness\n",
    "from tensorflow.keras.applications import EfficientNetV2B2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "import os"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "dataset_path = \"trashnet_dataset\"\n",
    "train_ds_raw = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=42,\n",
    "    image_size=(128, 128),\n",
    "    batch_size=32\n",
    ")\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=42,\n",
    "    image_size=(128, 128),\n",
    "    batch_size=32\n",
    ")\n",
    "class_names = train_ds_raw.class_names\n",
    "print(\"Classes:\", class_names)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compute class weights\n",
    "all_labels = [label.numpy() for _, label in train_ds_raw.unbatch()]\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.arange(len(class_names)),\n",
    "    y=all_labels\n",
    ")\n",
    "class_weights = {i: weight for i, weight in enumerate(class_weights_array)}\n",
    "print(\"Class Weights:\", class_weights)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Augmentation\n",
    "standard_aug = Sequential([\n",
    "    RandomFlip(\"horizontal\"),\n",
    "    RandomRotation(0.1),\n",
    "    RandomZoom(0.1)\n",
    "])\n",
    "strong_aug = Sequential([\n",
    "    RandomFlip(\"horizontal\"),\n",
    "    RandomRotation(0.2),\n",
    "    RandomZoom(0.2),\n",
    "    RandomContrast(0.2),\n",
    "    RandomBrightness(0.2)\n",
    "])\n",
    "def augment_standard(x, y): return standard_aug(x), y\n",
    "def augment_strong(x, y): return strong_aug(x), y\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Oversample trash class (index 5)\n",
    "trash_ds = (\n",
    "    train_ds_raw.unbatch()\n",
    "    .filter(lambda x, y: tf.equal(y, 5))\n",
    "    .map(augment_strong)\n",
    "    .repeat(4)\n",
    "    .batch(32)\n",
    ")\n",
    "non_trash_ds = (\n",
    "    train_ds_raw.unbatch()\n",
    "    .filter(lambda x, y: tf.not_equal(y, 5))\n",
    "    .map(augment_standard)\n",
    "    .batch(32)\n",
    ")\n",
    "train_ds = non_trash_ds.concatenate(trash_ds)\n",
    "train_ds = train_ds.shuffle(1000).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(\"Data preprocessing and oversampling complete!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load EfficientNetV2B2\n",
    "base_model = EfficientNetV2B2(include_top=False, input_shape=(128, 128, 3), weights=\"imagenet\")\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:200]:\n",
    "    layer.trainable = False\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dropout(0.3),\n",
    "    Dense(6, activation=\"softmax\")\n",
    "])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compile model\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-4,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    loss=SparseCategoricalFocalLoss(gamma=2.0),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "model.save(\"garbage_classifier_focal.keras\")\n",
    "print(\"Model saved successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot training curves\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(acc, label=\"Train Acc\")\n",
    "plt.plot(val_acc, label=\"Val Acc\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss, label=\"Train Loss\")\n",
    "plt.plot(val_loss, label=\"Val Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluate and visualize\n",
    "model = tf.keras.models.load_model(\"garbage_classifier_focal.keras\", custom_objects={\"SparseCategoricalFocalLoss\": SparseCategorical